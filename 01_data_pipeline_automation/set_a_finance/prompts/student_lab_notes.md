# Student Lab Notes â€” Session 01
## The Data Integrity Lesson: Why Pydantic Comes Before the LLM

---

> **Core Principle:** _An LLM is only as trustworthy as the data you feed it. Garbage in, garbage out â€” at enterprise scale._

---

## The Problem: LLMs Are Optimistic

Large Language Models are trained to be helpful. When you send them messy, malformed, or ambiguous data, they do not crash â€” they **hallucinate a plausible answer**. This is catastrophic in a compliance pipeline where a wrong category (`Policy-Compliant` instead of `Suspicious`) can cost an enterprise millions.

### Example of the failure mode:

| Input sent to LLM | LLM Response | Reality |
|---|---|---|
| `amount_usd: "N/A"` | `"Policy-Compliant â€” routine expense"` | The field was broken |
| `description: " "` | `"Needs Review â€” insufficient detail"` | The row was corrupt |
| `amount_usd: -450.00` | `"Policy-Compliant â€” small credit"` | Invalid negative value |

The LLM gave a confident, formatted answer every time â€” and was wrong every time.

---

## The Solution: Pydantic as the Gatekeeper

Pydantic enforces a **strict contract** on your data _before_ it touches the LLM. Think of it as a bouncer at the door of your AI system.

```
RAW CSV DATA
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pydantic Schema Guard     â”‚  â† Phase 1: Deterministic
â”‚                             â”‚
â”‚  âœ… Type coercion           â”‚
â”‚  âœ… Required field checks   â”‚
â”‚  âœ… Validator logic         â”‚
â”‚  âœ… Threshold rules         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚  Only clean, validated rows pass
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama LLM  (llama3.2)    â”‚  â† Phase 2: Probabilistic
â”‚                             â”‚
â”‚  ğŸ§  Description â†’ Category  â”‚
â”‚  ğŸ§  Context-aware reasoning â”‚
â”‚  ğŸ§  Nuanced edge cases      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
       HYBRID AUDIT REPORT
```

---

## The Two-Phase Architecture

### [INTEGRATOR] Perspective
You are integrating two systems that have **different failure modes**:

- **Rules** fail loudly â€” a Pydantic `ValidationError` crashes immediately and tells you exactly what is wrong.
- **LLMs** fail silently â€” a miscategorisation looks like a valid response.

By running rules _first_, you guarantee that the LLM only ever receives well-formed data. The LLM's job is then purely interpretive â€” it handles the ambiguity that rules cannot.

### [ARCHITECT] Perspective
This is the **Defence in Depth** pattern applied to AI pipelines:

| Layer | Tool | Type | Failure Mode |
|---|---|---|---|
| Layer 1 | Pydantic | Deterministic | Loud (exception) |
| Layer 2 | Threshold Rule | Deterministic | Loud (conditional) |
| Layer 3 | Ollama LLM | Probabilistic | Silent (needs logging) |

Each layer catches what the previous layer cannot. No single layer is trusted alone.

---

## Why This Matters for Sovereign AI

In an enterprise context, you may be running models **on-premises** (using Ollama instead of a cloud API). This means:

1. **Data never leaves your infrastructure** â€” Pydantic validation ensures only schema-clean data is even processed.
2. **Audit trails are complete** â€” every rejected row is logged with a specific reason _before_ any AI inference occurs.
3. **Reproducibility** â€” rule-based flags are 100% deterministic and can be re-run identically. LLM results are probabilistic but bounded by validated inputs.

---

## Key Takeaways

- âœ… **Always validate before you infer.** Pydantic is your data contract.
- âœ… **Deterministic rules catch known bad patterns.** LLMs catch unknown, nuanced ones.
- âœ… **A hybrid pipeline is more robust than either approach alone.**
- âœ… **Silent AI failures are more dangerous than loud rule failures.**

---

## Hands-On Check

Before moving to Session 02, confirm you can answer:

1. What happens if you send a row with a missing `transaction_id` directly to the LLM?
2. Why does `amount_usd` need to be coerced to `float` _before_ Pydantic validation?
3. What is the risk of using _only_ the LLM without the threshold rule?

---

_Session 01 Â· AI Bootcamps Â· Sovereign Data Pipeline_
_Persona: Supportive Facilitator Â· Theme: Cyber-Sovereign_
